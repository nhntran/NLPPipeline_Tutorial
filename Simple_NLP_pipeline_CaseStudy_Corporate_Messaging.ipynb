{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIMPLE NLP PIPELINE USING SKLEARN\n",
    "\n",
    "# CASE STUDY: CORPORATE MESSAGING\n",
    "\n",
    "### by Tran Nguyen\n",
    "\n",
    "This is a comprehensive tutorial/template for NLP pipeline\n",
    "Ref: Part of the materials is from the Data Science Nanodegree from Udacity.\n",
    "\n",
    "**DATA**: A csv file `corporate_messaging.csv` containing the text messages from all the corporations on the social media, which were classified into 4 different categories: \n",
    "- Information: objective statements about the company or its activities\n",
    "- Action: messages that ask for votes or ask users to click on links\n",
    "- Dialogue: replies to users\n",
    "- Exclude\n",
    "\n",
    "**GOAL**: Classify a text message into a specific category.\n",
    "\n",
    "**CONTENT**: This notebook includes 3 parts:\n",
    "\n",
    "- Part 1. Create the workflow for the Machine Learning, which includes loading data, processing and transforming data, fitting and predicting data and finally, displaying the result.\n",
    "- Part 2. Refactor the task into function to automate the workflow in part 1.\n",
    "- Part 3. Using pipeline for the workflow from Part 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. CREATE THE WORKFLOW\n",
    "\n",
    "### 1.1 LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"corporate_messaging.csv\")\n",
    "# => UnicodeDecodeError: 'utf-8' codec can't decode bytes in position 42-43: invalid continuation byte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good encoding name iso8859_4\n",
      "Good encoding name cp852\n",
      "Good encoding name mac_latin2\n",
      "Good encoding name cp865\n",
      "Good encoding name kz1048\n",
      "Good encoding name cp775\n",
      "Good encoding name cp864\n",
      "Good encoding name cp858\n",
      "Good encoding name iso8859_14\n",
      "Good encoding name koi8_r\n",
      "Good encoding name iso8859_2\n",
      "Good encoding name iso8859_10\n",
      "Good encoding name cp862\n",
      "Good encoding name cp866\n",
      "Good encoding name mac_iceland\n",
      "Good encoding name mac_roman\n",
      "Good encoding name iso8859_15\n",
      "Good encoding name iso8859_13\n",
      "Good encoding name cp1251\n",
      "Good encoding name cp1256\n",
      "Good encoding name latin_1\n",
      "Good encoding name iso8859_5\n",
      "Good encoding name cp855\n",
      "Good encoding name ptcp154\n",
      "Good encoding name cp850\n",
      "Good encoding name cp863\n",
      "Good encoding name iso8859_9\n",
      "Good encoding name iso8859_16\n",
      "Good encoding name cp1125\n",
      "Good encoding name cp861\n",
      "Good encoding name cp857\n",
      "Good encoding name cp437\n",
      "Good encoding name mac_turkish\n",
      "Good encoding name mac_greek\n",
      "Good encoding name mac_cyrillic\n",
      "Good encoding name cp860\n",
      "Good encoding name hp_roman8\n"
     ]
    }
   ],
   "source": [
    "### check where is the file of the encodings\n",
    "#from encodings import aliases\n",
    "#aliases.__file__\n",
    "\n",
    "## import all the encoding names\n",
    "from encodings.aliases import aliases\n",
    "alias_values = set(aliases.values())\n",
    "\n",
    "### Iterate through the alias_values list trying out the different encodings to see which one or ones work\n",
    "# Use a try - except statement. Otherwise your code will produce an error csv file with the wrong encoding.\n",
    "for encoding_name in alias_values:\n",
    "    try: \n",
    "        df = pd.read_csv('corporate_messaging.csv', encoding = encoding_name)\n",
    "        print(\"Good encoding name\", encoding_name)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xn7UWaD2Lo1_"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>_golden</th>\n",
       "      <th>_unit_state</th>\n",
       "      <th>_trusted_judgments</th>\n",
       "      <th>_last_judgment_at</th>\n",
       "      <th>category</th>\n",
       "      <th>category:confidence</th>\n",
       "      <th>category_gold</th>\n",
       "      <th>id</th>\n",
       "      <th>screenname</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>662822308</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>2/18/15 4:31</td>\n",
       "      <td>Information</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.365280e+17</td>\n",
       "      <td>Barclays</td>\n",
       "      <td>Barclays CEO stresses the importance of regula...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>662822309</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>2/18/15 13:55</td>\n",
       "      <td>Information</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.860130e+17</td>\n",
       "      <td>Barclays</td>\n",
       "      <td>Barclays announces result of Rights Issue http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>662822310</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>2/18/15 8:43</td>\n",
       "      <td>Information</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.795800e+17</td>\n",
       "      <td>Barclays</td>\n",
       "      <td>Barclays publishes its prospectus for its åŖ5....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    _unit_id  _golden _unit_state  _trusted_judgments _last_judgment_at  \\\n",
       "0  662822308    False   finalized                   3      2/18/15 4:31   \n",
       "1  662822309    False   finalized                   3     2/18/15 13:55   \n",
       "2  662822310    False   finalized                   3      2/18/15 8:43   \n",
       "\n",
       "      category  category:confidence category_gold            id screenname  \\\n",
       "0  Information                  1.0           NaN  4.365280e+17   Barclays   \n",
       "1  Information                  1.0           NaN  3.860130e+17   Barclays   \n",
       "2  Information                  1.0           NaN  3.795800e+17   Barclays   \n",
       "\n",
       "                                                text  \n",
       "0  Barclays CEO stresses the importance of regula...  \n",
       "1  Barclays announces result of Rights Issue http...  \n",
       "2  Barclays publishes its prospectus for its åŖ5....  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"corporate_messaging.csv\", encoding = 'iso8859_4')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3118, 11)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Information    2129\n",
       "Action          724\n",
       "Dialogue        226\n",
       "Exclude          39\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "Action         [0.6747, 1.0, 0.6634, 0.6775, 0.6695, 0.664, 0...\n",
       "Dialogue       [0.6606, 0.6666, 0.6747, 0.6628, 0.6695, 1.0, ...\n",
       "Exclude        [0.33899999999999997, 0.3366, 0.6622, 1.0, 0.6...\n",
       "Information    [1.0, 0.6573, 0.6643, 0.6569, 0.6656, 0.6614, ...\n",
       "Name: category:confidence, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"category\")[\"category:confidence\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_highconfidence = df[df[\"category:confidence\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Information    1823\n",
       "Action          456\n",
       "Dialogue        124\n",
       "Exclude          27\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_highconfidence.category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of data that has confidence value == 1:\n",
      "Information 85.62705495537811\n",
      "Action 62.98342541436463\n",
      "Dialogue 54.86725663716814\n",
      "Exclude 69.23076923076923\n"
     ]
    }
   ],
   "source": [
    "### Percentage of data that has confidence value == 1 in each category\n",
    "names = df.category.value_counts().index\n",
    "vals = df.category.value_counts().values\n",
    "print(\"Percentage of data that has confidence value == 1:\")\n",
    "for i in range(len(names)):\n",
    "    print(names[i], df_highconfidence.category.value_counts()[i]/vals[i]*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=> Choose the data that has confidence value == 1 and only choose the 3 main categories.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Information    1823\n",
       "Action          456\n",
       "Dialogue        124\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = df[(df[\"category:confidence\"] == 1) & (df[\"category\"] != \"Exclude\")]\n",
    "df1.category.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate X (text) and Y (category) as numpy array**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2403 2403\n"
     ]
    }
   ],
   "source": [
    "X = df1.text.values\n",
    "y = df1.category.values\n",
    "print(len(X), len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Information', 'Information', 'Information', 'Information',\n",
       "       'Information'], dtype=object)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Barclays CEO stresses the importance of regulatory and cultural reform in financial services at Brussels conference  http://t.co/Ge9Lp7hpyG',\n",
       "       'Barclays announces result of Rights Issue http://t.co/LbIqqh3wwG',\n",
       "       'Barclays publishes its prospectus for its åŖ5.8bn Rights Issue: http://t.co/YZk24iE8G6',\n",
       "       'Barclays Group Finance Director Chris Lucas is to step down at the end of the week due to ill health http://t.co/nkuHoAfnSD',\n",
       "       'Barclays announces that Irene McDermott Brown has been appointed as Group Human Resources Director http://t.co/c3fNGY6NMT'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. PREPARE DATA\n",
    "\n",
    "#### 1.2.1. PROCESS THE X VALUES (TEXT DATA) \n",
    "\n",
    "- **To-do list**:\n",
    "    + replace url with the common name for url such as \"urlplaceholder\".\n",
    "    + split text into tokens\n",
    "    + Processing: normalize case, strip white space, lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barclays Group Finance Director Chris Lucas is to step down at the end of the week due to ill health urlplaceholder\n"
     ]
    }
   ],
   "source": [
    "#### url replace:\n",
    "url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "\n",
    "text = re.sub(url_regex, 'urlplaceholder', X[3])\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/nhntran/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nhntran/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/nhntran/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "stop_words = stopwords.words(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!!! It doesn't work if we tokenize the data this way!!! Why?\n",
    "\n",
    "# def tokenize(text):\n",
    "#     \"\"\" Function to process text:\n",
    "#         + replace url with the common name for url \"urlplaceholder\"\n",
    "#         + normalize case, remove punctuation\n",
    "#         + tokenize text\n",
    "#         + lemmatize and remove stop words\n",
    "#     \"\"\"\n",
    "#     url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "\n",
    "#     text = re.sub(url_regex, 'urlplaceholder', text)\n",
    "#     text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
    "#     tokens = word_tokenize(text)\n",
    "#     #tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "#     tokens = [lemmatizer.lemmatize(word).strip() for word in tokens]\n",
    "#     return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    detected_urls = re.findall(url_regex, text)\n",
    "    for url in detected_urls:\n",
    "        text = text.replace(url, \"urlplaceholder\")\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    clean_tokens = []\n",
    "    for tok in tokens:\n",
    "        clean_tok = lemmatizer.lemmatize(tok).lower().strip()\n",
    "        clean_tokens.append(clean_tok)\n",
    "\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['barclays',\n",
       " 'publishes',\n",
       " 'it',\n",
       " 'prospectus',\n",
       " 'for',\n",
       " 'it',\n",
       " 'åŗ5.8bn',\n",
       " 'rights',\n",
       " 'issue',\n",
       " ':',\n",
       " 'urlplaceholder']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenize(X[2])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2. SPLIT DATA INTO TRAIN AND TEST SET\n",
    "\n",
    "- Using the function `train_test_split` from `sklearn.model_selection`: Split train:test as 75%:25%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1802 601 1802 601\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "print(len(X_train), len(X_test), len(y_train), len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3. TRAIN CLASSIFIER\n",
    "\n",
    "##### 1.2.3.1. VECTORIZE TEXT DATA USING BAG OF WORDS AND TF-IDF VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "# initialize count vectorizer object\n",
    "vect = CountVectorizer(tokenizer = tokenize)\n",
    "# initialize tf-idf transformer object\n",
    "transformer = TfidfTransformer()\n",
    "\n",
    "#### Transform X_train data\n",
    "# Get counts of each word\n",
    "X_train_counts = vect.fit_transform(X_train)\n",
    "# use counts from count vectorizer results to compute tf-idf values\n",
    "X_train_tfidf = transformer.fit_transform(X_train_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.3.2. TRAIN CLASSIFIER USING RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# initialize the random forest classifier\n",
    "clf = RandomForestClassifier()\n",
    "# fit X_train using random forest\n",
    "clf.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.4. PREDICT ON TEST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Transform X_test data\n",
    "# get counts of each word\n",
    "X_test_counts = vect.transform(X_test)\n",
    "# use counts from count vectorizer results to compute tf-idf values\n",
    "X_test_tfidf = transformer.transform(X_test_counts)\n",
    "#c.fit(X_train.values.reshape(-1, 1), y_train)\n",
    "#### Predict on test data\n",
    "y_pred = clf.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#HealthyBytes 8: Take a guess! Does sleep deprivation make you fatter or slimmer? Watch the video http://t.co/7qOBZz0tSa'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Action'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.5. EVALUATE THE ACCURACY OF A CLASSIFICATION\n",
    "\n",
    "- Using the confusion matrix\n",
    "- Using `confusion_matrix` function from `sklearn.metrics`\n",
    "- Confusion matrix example:\n",
    " Cj,j = the number of observations known to be in group i and predicted to be in group j.\n",
    "\n",
    "                       'Action', 'Dialogue', 'Information'\n",
    "       'Action'           87        0            26\n",
    "       'Dialogue'          3        21            8\n",
    "       'Information'       4         0            452\n",
    "\n",
    "- In binary classification, the count of true negatives is C0,0, false negatives is C1,0, true positives is C1,1 and false positives is C0,1.\n",
    "                        False    True\n",
    "               False      5        5\n",
    "               True      10       80\n",
    "- tn, fp, fn, tp coul be extract using `ravel` function:\n",
    "    \n",
    "    Example: tn, fp, fn, tp = confusion_matrix([0, 1, 0, 1], [1, 1, 1, 0]).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Action', 'Dialogue', 'Information'], dtype=object)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = np.unique(y_pred)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "['Action' 'Dialogue' 'Information']\n",
      "[[ 86   0  15]\n",
      " [  0  28   7]\n",
      " [  4   1 460]]\n",
      "Accuracy: 0.9550748752079867\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "# calculate the confusion matrix from y_test and y_pred\n",
    "confusion_mat = confusion_matrix(y_test, y_pred, labels = labels)\n",
    "print(\"Confusion matrix:\")\n",
    "print(labels)\n",
    "print(confusion_mat)\n",
    "# calculate the accuracy\n",
    "accuracy = (y_pred == y_test).mean()\n",
    "# or accuracy = sum(y_pred == y_test)/len(y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. REFACTOR TO AUTOMATE THE WORKFLOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/nhntran/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nhntran/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/nhntran/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "['Action' 'Dialogue' 'Information']\n",
      "[[ 88   0  21]\n",
      " [  0  23   5]\n",
      " [  4   0 460]]\n",
      "Accuracy: 0.9500831946755408\n"
     ]
    }
   ],
   "source": [
    "#### Import the neccessary Python packages\n",
    "# basic packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# processing the data\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# transform and fit data\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# evaluate the result\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#### function to be use\n",
    "def load_data():\n",
    "    \"\"\" Load data from csv file\n",
    "        Filter the data and get the X, y data\n",
    "        return: X, y\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(\"corporate_messaging.csv\", encoding = 'iso8859_4')\n",
    "    \n",
    "    #  The 4 categories: number of samples are: \n",
    "    # Information: 2129, Action: 724, Dialogue: 226, Exclude: 39\n",
    "    # Choose the data that has confidence value == 1 and only choose the 3 main categories.\n",
    "    df1 = df[(df[\"category:confidence\"] == 1) & (df[\"category\"] != \"Exclude\")]\n",
    "    X = df1.text.values\n",
    "    y = df1.category.values\n",
    "    return X, y\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\" Function to process text:\n",
    "         + replace url with the common name for url \"urlplaceholder\"\n",
    "         + normalize case, remove punctuation\n",
    "         + tokenize text\n",
    "         + lemmatize words\n",
    "     \"\"\"\n",
    "    url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    detected_urls = re.findall(url_regex, text)\n",
    "    for url in detected_urls:\n",
    "        text = text.replace(url, \"urlplaceholder\")\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    clean_tokens = [lemmatizer.lemmatize(tok).lower().strip() for tok in tokens]\n",
    "\n",
    "    return clean_tokens\n",
    "\n",
    "def display_results(y_test, y_pred):\n",
    "    \"\"\" Display the confusion matrix and accuracy\n",
    "    \"\"\"\n",
    "    labels = np.unique(y_pred)\n",
    "    # calculate the confusion matrix from y_test and y_pred\n",
    "    confusion_mat = confusion_matrix(y_test, y_pred, labels = labels)\n",
    "    print(\"Confusion matrix:\")\n",
    "    print(labels)\n",
    "    print(confusion_mat)\n",
    "    accuracy = (y_pred == y_test).mean()\n",
    "    # or accuracy = sum(y_pred == y_test)/len(y_pred)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "\n",
    "def main():\n",
    "    ## prepare train and test set\n",
    "    X, y = load_data()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    \n",
    "    ## prepare transformers and estimator\n",
    "    # initialize count vectorizer object\n",
    "    vect = CountVectorizer(tokenizer = tokenize)\n",
    "    # initialize tf-idf transformer object\n",
    "    transformer = TfidfTransformer()\n",
    "    # initialize the random forest classifier\n",
    "    clf = RandomForestClassifier()\n",
    "    \n",
    "    #### Transform X_train data\n",
    "    # Get counts of each word\n",
    "    X_train_counts = vect.fit_transform(X_train)\n",
    "    # use counts from count vectorizer results to compute tf-idf values\n",
    "    X_train_tfidf = transformer.fit_transform(X_train_counts)\n",
    "    # fit X_train using random forest\n",
    "    clf.fit(X_train_tfidf, y_train)\n",
    "    \n",
    "    #### Transform X_test data\n",
    "    # get counts of each word\n",
    "    X_test_counts = vect.transform(X_test)\n",
    "    # use counts from count vectorizer results to compute tf-idf values\n",
    "    X_test_tfidf = transformer.transform(X_test_counts)\n",
    "    #c.fit(X_train.values.reshape(-1, 1), y_train)\n",
    "    #### Predict on test data\n",
    "    y_pred = clf.predict(X_test_tfidf)\n",
    "    \n",
    "    display_results(y_test, y_pred)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. IMPLEMENT PIPELINE INTO THE WORKFLOW\n",
    "\n",
    "\n",
    "- Use the `Pipeline` from `sklearn.pipeline`.\n",
    "\n",
    "- **HOW TO USE PIPELINE**:\n",
    "    \n",
    "    + Define a pipeline: pipeline = Pipeline(`[a list of transformers and 1 estimator at the end]`)\n",
    "    + Train the data: pipeline.fit(X_train, y_train)\n",
    "    + Predict: y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ADVANTAGE OF USING PIPELINE**:\n",
    "\n",
    "- Automate repetitive steps\n",
    "- Easy to understand\n",
    "- Easy to optimize the workflow (tuning the parameters)\n",
    "- Prevent data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/nhntran/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nhntran/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/nhntran/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "['Action' 'Dialogue' 'Information']\n",
      "[[ 82   1  29]\n",
      " [  1  23   4]\n",
      " [  5   0 456]]\n",
      "Accuracy: 0.9334442595673876\n"
     ]
    }
   ],
   "source": [
    "#### Import the neccessary Python packages\n",
    "# basic packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "# processing the data\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from sklearn.model_selection import train_test_split\n",
    "# transform and fit data\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# evaluate the result\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# add the pipeline for using pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#### function to be use\n",
    "def load_data():\n",
    "    \"\"\" Load data from csv file\n",
    "        Filter the data and get the X, y data\n",
    "        return: X, y\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(\"corporate_messaging.csv\", encoding = 'iso8859_4')\n",
    "    \n",
    "    #  The 4 categories: number of samples are: \n",
    "    # Information: 2129, Action: 724, Dialogue: 226, Exclude: 39\n",
    "    # Choose the data that has confidence value == 1 and only choose the 3 main categories.\n",
    "    df1 = df[(df[\"category:confidence\"] == 1) & (df[\"category\"] != \"Exclude\")]\n",
    "    X = df1.text.values\n",
    "    y = df1.category.values\n",
    "    return X, y\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\" Function to process text:\n",
    "         + replace url with the common name for url \"urlplaceholder\"\n",
    "         + normalize case, remove punctuation\n",
    "         + tokenize text\n",
    "         + lemmatize words\n",
    "     \"\"\"\n",
    "    url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    detected_urls = re.findall(url_regex, text)\n",
    "    for url in detected_urls:\n",
    "        text = text.replace(url, \"urlplaceholder\")\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    clean_tokens = [lemmatizer.lemmatize(tok).lower().strip() for tok in tokens]\n",
    "\n",
    "    return clean_tokens\n",
    "\n",
    "def display_results(y_test, y_pred):\n",
    "    \"\"\" Display the confusion matrix and accuracy\n",
    "    \"\"\"\n",
    "    labels = np.unique(y_pred)\n",
    "    # calculate the confusion matrix from y_test and y_pred\n",
    "    confusion_mat = confusion_matrix(y_test, y_pred, labels = labels)\n",
    "    print(\"Confusion matrix:\")\n",
    "    print(labels)\n",
    "    print(confusion_mat)\n",
    "    accuracy = (y_pred == y_test).mean()\n",
    "    # or accuracy = sum(y_pred == y_test)/len(y_pred)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "\n",
    "def main():\n",
    "    ## prepare train and test set\n",
    "    X, y = load_data()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    \n",
    "    ## initialize pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer = tokenize)),\n",
    "        ('transformer', TfidfTransformer()),\n",
    "        ('clf',RandomForestClassifier())\n",
    "    ])\n",
    "    \n",
    "    ## train classifier\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    #### Predict on test data\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    display_results(y_test, y_pred)\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "cleaning_solution (1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
